# -*- coding: utf-8 -*-
"""sensitivityLCRsurface_MF_wss_cogpr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TBBXBsRd40LT5_n7NnyI697zCaq6LaKI
"""

pip install GPy

pip install emukit

pip install pyDOE

import GPy
import numpy as np
from pyDOE import lhs

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd


ALL_xdata=pd.read_csv('/content/drive/MyDrive/ColabNotebooks/scaled_L2_4_dns.csv', header=None)
# Full length
# DNS_ydata=pd.read_csv('/content/drive/MyDrive/ColabNotebooks/scaled_wss_dns11data.csv', header=None)

DNS_ydata=pd.read_csv('/content/drive/MyDrive/ColabNotebooks/scaled_wss_dns11_L2t4data.csv', header=None)
RANS_ydata=pd.read_csv('/content/drive/MyDrive/ColabNotebooks/scaled_wss_LCrans_all_L2t4data.csv',header=None)



# Generate sample data

x_train_l =  np.array([[70,100],[70,80],[70,50],[70,30],[70,0],[60,100],[60,80],[60,50],[60,30],[60,0],[50,100],[50,80],[50,50],[50,30],[50,0],[40,100],[40,80],[40,50],[40,30],[40,0]])
x_train_h = np.array([[70,100],[70,0],[60,80],[50,50],[40,100],[40,0]])

DNS_full=DNS_ydata.dropna(axis=1)
DNS_full=DNS_full.values
DNS=DNS_full[:,[0,2,3,7,8,10]]


DNS_test=DNS_full[:,[1,4,5,6,9]]
DNS_test=np.transpose(DNS_test)
RANS=RANS_ydata.dropna(axis=1)
RANS=RANS.values
RANS_full=RANS

RANS_test=RANS_full[:,[0,1,3,4,5,6,7,10,12,13,14,15,16,17,19]]

# Max value of the WSS

# Find the maximum value in each column
RANS= np.max(RANS, axis=0)

RANS_full=np.max(RANS, axis=0)

RANS_test=np.max(RANS_test, axis=0)

DNS= np.max(DNS, axis=0)

DNS_full=np.max(DNS_full, axis=0)

DNS_test=np.max(DNS_test, axis=0)

y_train_h=np.transpose(DNS)
y_train_l=np.transpose(RANS)

y_train_h=y_train_h.reshape(-1,1)
y_train_l=y_train_l.reshape(-1,1)

print(y_train_h)
print(y_train_l)

# x_train_l=np.c_[ x_train_l, np.zeros(len(x_train_l)) ]
# x_train_h=np.c_[ x_train_h, np.ones(len(x_train_h)) ]
# X_train=np.append(x_train_l,x_train_h,axis=0)
#
from emukit.multi_fidelity.convert_lists_to_array import convert_x_list_to_array, convert_xy_lists_to_arrays

# Y_train=np.append(y_train_h,y_train_l,axis=0)
X_train, Y_train = convert_xy_lists_to_arrays([x_train_l, x_train_h], [y_train_l, y_train_h])
print(X_train.shape)
print(Y_train.shape)

print(X_train)

# Commented out IPython magic to ensure Python compatibility.
import GPy
import emukit.multi_fidelity
import emukit.test_functions
from emukit.model_wrappers.gpy_model_wrappers import GPyMultiOutputWrapper
from emukit.multi_fidelity.models import GPyLinearMultiFidelityModel
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import colors as mcolors
colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)
# %matplotlib inline

## Create standard GP model using only high-fidelity data
x_train_DNS=x_train_h
y_train_DNS=y_train_h
kernelDNS = GPy.kern.ExpQuad(input_dim=2,lengthscale=[10,5.0],variance=0.0001, ARD=True)
kernelDNS.lengthscale.constrain_bounded(0,10000)
kernelDNS.variance.constrain_bounded(0,10)
high_gp_model = GPy.models.GPRegression(x_train_DNS, y_train_DNS, kernelDNS)
high_gp_model.Gaussian_noise.unfix()

## Fit the GP model

high_gp_model.optimize_restarts(100)

print(high_gp_model.ExpQuad.lengthscale[0])
print(high_gp_model.ExpQuad.lengthscale[1])
print(high_gp_model.ExpQuad.variance)
print(high_gp_model.gradient)
# Get the gradients of optimization
gradients_dns = high_gp_model.gradient

# Get the names of the model's parameters
parameter_dns = high_gp_model.parameter_names()

# Print the gradients along with the parameter names
print("Gradients of Optimization:")
for i in range(len(parameter_dns)):
    print(f"Parameter: {parameter_dns[i]} - Gradient: {gradients_dns[i]}")

## Create standard GP model using only low-fidelity data

x_train_RANS= np.array([[70,100],[70,80],[70,30],[70,0],[60,100],[60,80],[60,50],[50,100],[50,50],[50,30],[50,0],[40,100],[40,80],[40,50],[40,0]])

y_train_RANS=np.transpose(RANS_test)
y_train_RANS=y_train_RANS.reshape(-1,1)
print(y_train_RANS.shape)

kernelRANS = GPy.kern.ExpQuad(input_dim=2,lengthscale=[10,10],variance=0.05, ARD=True)
kernelRANS.lengthscale.constrain_bounded(0,1000)
kernelRANS.variance.constrain_bounded(0,10)
low_gp_model = GPy.models.GPRegression(x_train_RANS, y_train_RANS, kernelRANS)
low_gp_model.Gaussian_noise.unfix()

## Fit the GP model

low_gp_model.optimize_restarts(500)

#x_plot=np.reshape(x,[10000,2])
x_plot=np.array([[50,50]])
hf_mean_high_gp_model, hf_var_high_gp_model  = high_gp_model.predict(x_plot)

print(low_gp_model.ExpQuad.lengthscale[0])
print(low_gp_model.ExpQuad.lengthscale[1])
print(low_gp_model.ExpQuad.variance)

# Get the gradients of optimization
gradients_rans = low_gp_model.gradient

# Get the names of the model's parameters
parameter_rans = low_gp_model.parameter_names()

# Print the gradients along with the parameter names
print("Gradients of Optimization:")
for i in range(len(parameter_rans)):
    print(f"Parameter: {parameter_rans[i]} - Gradient: {gradients_rans[i]}")

kernels = [GPy.kern.ExpQuad(input_dim=2,lengthscale=[10,10],variance=0.005, ARD=True),GPy.kern.ExpQuad(input_dim=2,lengthscale=[10,10],variance=0.005, ARD=True)]
# for kernel in kernels:
#     kernel.lengthscale.constrain_bounded(0, 50)
#     kernel.variance.constrain_bounded(0, 10)
for i, kernel in enumerate(kernels):
    if i == 0:  # First fidelity
        kernel.lengthscale.constrain_bounded(0, 25)
        kernel.variance.constrain_bounded(0, 5)
    elif i == 1:  # Second fidelity
        kernel.lengthscale.constrain_bounded(0, 25)
        kernel.variance.constrain_bounded(0, 10)
lin_mf_kernel = emukit.multi_fidelity.kernels.LinearMultiFidelityKernel(kernels)

gpy_lin_mf_model = GPyLinearMultiFidelityModel(X_train, Y_train, lin_mf_kernel, n_fidelities=2)

# gpy_lin_mf_model.mixed_noise.Gaussian_noise.unfix()
# gpy_lin_mf_model.mixed_noise.Gaussian_noise_1.fix(0.001)

lin_mf_model =model=GPyMultiOutputWrapper(gpy_lin_mf_model, 2, n_optimization_restarts=50)

## Fit the model

lin_mf_model.optimize()



print(gpy_lin_mf_model)

print(gpy_lin_mf_model.multifidelity.ExpQuad.variance)
  print(gpy_lin_mf_model.multifidelity.ExpQuad_1.variance)
  print(gpy_lin_mf_model.multifidelity.ExpQuad.lengthscale)
  print(gpy_lin_mf_model.multifidelity.ExpQuad_1.lengthscale)

# Get the gradients of optimization
gradients_mf = gpy_lin_mf_model.multifidelity.gradient

# Get the names of the model's parameters
parameter_mf = gpy_lin_mf_model.multifidelity.parameter_names()

# Print the gradients along with the parameter names
print("Gradients of Optimization:")
for i in range(len(parameter_mf)):
    print(f"Parameter: {parameter_mf[i]} - Gradient: {gradients_mf[i]}")

x_truth =  np.array([[70,100],[70,50],[70,0],[60,80],[60,30],[60,0],[50,80],[50,50],[40,100],[40,30],[40,0]])
y_truth=DNS_full.reshape(-1,1)
print(y_truth.shape)

truth_gp_model = GPy.models.GPRegression(x_truth, y_truth, kernelDNS)
truth_gp_model.Gaussian_noise.unfix()

## Fit the GP model

truth_gp_model.optimize_restarts(100)

#x_plot=np.reshape(x,[10000,2])
x_plot=np.array([[70,50],[60,30],[60,0],[50,80],[40,30]])
X_plot_l=np.c_[ x_plot, np.zeros(len(x_plot)) ]
X_plot_h=np.c_[ x_plot, np.ones(len(x_plot)) ]

hf_mean_lin_mf_model, hf_var_lin_mf_model = lin_mf_model.predict(X_plot_h)

hf_std_lin_mf_model = np.sqrt(hf_var_lin_mf_model)
hf_mean_high_gp_model, hf_var_high_gp_model  = high_gp_model.predict(X_plot_h)
lf_mean_low_gp_model, lf_var_low_gp_model  = low_gp_model.predict(X_plot_h)
tr_mean_truth_gp_model,tr_var_truth_gp_model=truth_gp_model.predict(X_plot_h)

pip install chaospy

import GPy
import chaospy as cp
import numpy as np

import numpy as np

def saltelli_indices(model, X, n_samples, n_params, ranges):
    """
    Perform Monte Carlo estimation of Sobol' indices using Saltelli scheme.

    Parameters:
        model (callable): Function that represents the model.
        X (ndarray): Array of input parameter values.
        n_samples (int): Number of samples to generate.
        n_params (int): Number of parameters.
        ranges (list): List of tuples specifying the ranges for each parameter.

    Returns:
        S (ndarray): First-order Sobol' indices.
        ST (ndarray): Total-effect Sobol' indices.
    """
    # Generate uniform samples within the specified ranges
    samples = np.zeros((n_samples, n_params))
    for j in range(n_params):
        samples[:, j] = np.random.uniform(ranges[j][0], ranges[j][1], n_samples)

    # Scale the input samples to the desired range
    input_samples_h = np.hstack((samples, np.ones((n_samples, 1))))
    print(input_samples_h)
    # Initialize arrays for model evaluations
    Y = np.zeros(n_samples)
    Ys = np.zeros((n_samples, n_params))

    # Evaluate model at sample points
    out_m, out_v = model(X, input_samples_h)
    Y = out_m[:, -1]  # Extract the last column from out_m

    for j in range(n_params):
        input_samples_h[:, j] = ranges[j][1] - (input_samples_h[:, j] - ranges[j][0])
        out_m, out_v = model(X, input_samples_h)
        Ys[:, j] = out_m[:, -1]  # Extract the last column from out_m
        input_samples_h[:, j] = ranges[j][1] - (input_samples_h[:, j] - ranges[j][0])

    # Calculate mean of model evaluations
    Y_mean = np.mean(Y)

    # Initialize arrays for indices
    S = np.zeros(n_params)
    ST = np.zeros(n_params)

    # Calculate indices
    for j in range(n_params):
        S[j] = np.mean(Y * (Ys[:, j] - Y_mean))

        # Calculate the normalization factor for the total-effect Sobol' indices
        norm_factor = 1 / np.var(Y)
        ST[j] = norm_factor * (np.mean((Y - Ys[:, j])**2) / np.var(Y))

    return S, ST

# Example usage
def model(X, params):
    # Example model function
    out_m = np.zeros((params.shape[0], X.shape[0]+1))
    out_v = np.zeros(params.shape[0])

    # Your model code here...
    out_m, out_v = lin_mf_model.predict(params)
    return out_m, out_v

# Define input parameter values
X = np.array([1, 2])

# Define number of samples and parameters
n_samples = 10000000
n_params = 2

# Define ranges for each parameter
ranges = [(40, 70), (0, 100)]

# Perform Monte Carlo estimation of Sobol' indices
S, ST = saltelli_indices(model, X, n_samples, n_params, ranges)

# Normalize the total-effect Sobol' indices
ST /= np.sum(ST)


print("Total-effect Sobol' indices:", ST)
print('MF-Prediction')



import numpy as np

def saltelli_indices(model, X, n_samples, n_params, ranges):
    """
    Perform Monte Carlo estimation of Sobol' indices using Saltelli scheme.

    Parameters:
        model (callable): Function that represents the model.
        X (ndarray): Array of input parameter values.
        n_samples (int): Number of samples to generate.
        n_params (int): Number of parameters.
        ranges (list): List of tuples specifying the ranges for each parameter.

    Returns:
        S (ndarray): First-order Sobol' indices.
        ST (ndarray): Total-effect Sobol' indices.
    """
    # Generate uniform samples within the specified ranges
    samples = np.zeros((n_samples, n_params))
    for j in range(n_params):
        samples[:, j] = np.random.uniform(ranges[j][0], ranges[j][1], n_samples)

    # Scale the input samples to the desired range
    input_samples_h = np.hstack((samples, np.zeros((n_samples, 1))))
    print(input_samples_h)
    # Initialize arrays for model evaluations
    Y = np.zeros(n_samples)
    Ys = np.zeros((n_samples, n_params))

    # Evaluate model at sample points
    out_m, out_v = model(X, input_samples_h)
    Y = out_m[:, -1]  # Extract the last column from out_m

    for j in range(n_params):
        input_samples_h[:, j] = ranges[j][1] - (input_samples_h[:, j] - ranges[j][0])
        out_m, out_v = model(X, input_samples_h)
        Ys[:, j] = out_m[:, -1]  # Extract the last column from out_m
        input_samples_h[:, j] = ranges[j][1] - (input_samples_h[:, j] - ranges[j][0])

    # Calculate mean of model evaluations
    Y_mean = np.mean(Y)

    # Initialize arrays for indices
    S = np.zeros(n_params)
    ST = np.zeros(n_params)

    # Calculate indices
    for j in range(n_params):
        S[j] = np.mean(Y * (Ys[:, j] - Y_mean))

        # Calculate the normalization factor for the total-effect Sobol' indices
        norm_factor = 1 / np.var(Y)
        ST[j] = norm_factor * (np.mean((Y - Ys[:, j])**2) / np.var(Y))

    return S, ST

# Example usage
def model(X, params):
    # Example model function
    out_m = np.zeros((params.shape[0], X.shape[0]+1))
    out_v = np.zeros(params.shape[0])

    # Your model code here...
    out_m, out_v = high_gp_model.predict(params)
    return out_m, out_v

# Define input parameter values
X = np.array([1, 2])

# Define number of samples and parameters
n_samples = 10000000
n_params = 2

# Define ranges for each parameter
ranges = [(40, 70), (0, 100)]

# Perform Monte Carlo estimation of Sobol' indices
S, ST = saltelli_indices(model, X, n_samples, n_params, ranges)

# Normalize the total-effect Sobol' indices
ST /= np.sum(ST)

# Print the results
print("Total-effect Sobol' indices:", ST)
print('DNS-GPR')

import numpy as np

def saltelli_indices(model, X, n_samples, n_params, ranges):
    """
    Perform Monte Carlo estimation of Sobol' indices using Saltelli scheme.

    Parameters:
        model (callable): Function that represents the model.
        X (ndarray): Array of input parameter values.
        n_samples (int): Number of samples to generate.
        n_params (int): Number of parameters.
        ranges (list): List of tuples specifying the ranges for each parameter.

    Returns:
        S (ndarray): First-order Sobol' indices.
        ST (ndarray): Total-effect Sobol' indices.
    """
    # Generate uniform samples within the specified ranges
    samples = np.zeros((n_samples, n_params))
    for j in range(n_params):
        samples[:, j] = np.random.uniform(ranges[j][0], ranges[j][1], n_samples)

    # Scale the input samples to the desired range
    input_samples_h = np.hstack((samples, np.zeros((n_samples, 1))))
    print(input_samples_h)
    # Initialize arrays for model evaluations
    Y = np.zeros(n_samples)
    Ys = np.zeros((n_samples, n_params))

    # Evaluate model at sample points
    out_m, out_v = model(X, input_samples_h)
    Y = out_m[:, -1]  # Extract the last column from out_m

    for j in range(n_params):
        input_samples_h[:, j] = ranges[j][1] - (input_samples_h[:, j] - ranges[j][0])
        out_m, out_v = model(X, input_samples_h)
        Ys[:, j] = out_m[:, -1]  # Extract the last column from out_m
        input_samples_h[:, j] = ranges[j][1] - (input_samples_h[:, j] - ranges[j][0])

    # Calculate mean of model evaluations
    Y_mean = np.mean(Y)

    # Initialize arrays for indices
    S = np.zeros(n_params)
    ST = np.zeros(n_params)

    # Calculate indices
    for j in range(n_params):
        S[j] = np.mean(Y * (Ys[:, j] - Y_mean))

        # Calculate the normalization factor for the total-effect Sobol' indices
        norm_factor = 1 / np.var(Y)
        ST[j] = norm_factor * (np.mean((Y - Ys[:, j])**2) / np.var(Y))

    return S, ST

# Example usage
def model(X, params):
    # Example model function
    out_m = np.zeros((params.shape[0], X.shape[0]+1))
    out_v = np.zeros(params.shape[0])

    # Your model code here...
    out_m, out_v =low_gp_model.predict(params)
    return out_m, out_v

# Define input parameter values
X = np.array([1, 2])

# Define number of samples and parameters
n_samples = 10000000
n_params = 2

# Define ranges for each parameter
ranges = [(40, 70), (0, 100)]

# Perform Monte Carlo estimation of Sobol' indices
S, ST = saltelli_indices(model, X, n_samples, n_params, ranges)

# Normalize the total-effect Sobol' indices
ST /= np.sum(ST)

# Print the results
print("Total-effect Sobol' indices:", ST)
print('RANS-GPR')

import numpy as np

def saltelli_indices(model, X, n_samples, n_params, ranges):
    """
    Perform Monte Carlo estimation of Sobol' indices using Saltelli scheme.

    Parameters:
        model (callable): Function that represents the model.
        X (ndarray): Array of input parameter values.
        n_samples (int): Number of samples to generate.
        n_params (int): Number of parameters.
        ranges (list): List of tuples specifying the ranges for each parameter.

    Returns:
        S (ndarray): First-order Sobol' indices.
        ST (ndarray): Total-effect Sobol' indices.
    """
    # Generate uniform samples within the specified ranges
    samples = np.zeros((n_samples, n_params))
    for j in range(n_params):
        samples[:, j] = np.random.uniform(ranges[j][0], ranges[j][1], n_samples)

    # Scale the input samples to the desired range
    input_samples_h = np.hstack((samples, np.zeros((n_samples, 1))))
    print(input_samples_h)
    # Initialize arrays for model evaluations
    Y = np.zeros(n_samples)
    Ys = np.zeros((n_samples, n_params))

    # Evaluate model at sample points
    out_m, out_v = model(X, input_samples_h)
    Y = out_m[:, -1]  # Extract the last column from out_m

    for j in range(n_params):
        input_samples_h[:, j] = ranges[j][1] - (input_samples_h[:, j] - ranges[j][0])
        out_m, out_v = model(X, input_samples_h)
        Ys[:, j] = out_m[:, -1]  # Extract the last column from out_m
        input_samples_h[:, j] = ranges[j][1] - (input_samples_h[:, j] - ranges[j][0])

    # Calculate mean of model evaluations
    Y_mean = np.mean(Y)

    # Initialize arrays for indices
    S = np.zeros(n_params)
    ST = np.zeros(n_params)

    # Calculate indices
    for j in range(n_params):
        S[j] = np.mean(Y * (Ys[:, j] - Y_mean))

        # Calculate the normalization factor for the total-effect Sobol' indices
        norm_factor = 1 / np.var(Y)
        ST[j] = norm_factor * (np.mean((Y - Ys[:, j])**2) / np.var(Y))

    return S, ST

# Example usage
def model(X, params):
    # Example model function
    out_m = np.zeros((params.shape[0], X.shape[0]+1))
    out_v = np.zeros(params.shape[0])

    # Your model code here...
    out_m, out_v =truth_gp_model.predict(params)
    return out_m, out_v

# Define input parameter values
X = np.array([1, 2])

# Define number of samples and parameters
n_samples = 10000000
n_params = 2

# Define ranges for each parameter
ranges = [(40, 70), (0, 100)]

# Perform Monte Carlo estimation of Sobol' indices
S, ST = saltelli_indices(model, X, n_samples, n_params, ranges)

# Normalize the total-effect Sobol' indices
ST /= np.sum(ST)

# Print the results
print("Total-effect Sobol' indices:", ST)
print('Truth')